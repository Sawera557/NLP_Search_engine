{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxBBfaZ0jZZm"
   },
   "source": [
    "\n",
    "\n",
    "# Query pre_processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXbiDj--nrRE",
    "outputId": "58b60d38-a03b-4c01-f6d7-de7c6e986da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ntw2JtM1jUyz",
    "outputId": "e4ee07aa-3a0f-449f-bb34-cb7cb5abdc35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Read/create the text data\n",
    "#tweet_sample= \"How to take control of your #debt https://personal.vanguard.com/us/insights/saving-investing/ debt-management.#Best advice for #family #financial #success (@PrepareToWin)\"\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    if text.startswith(\" \"):\n",
    "        text = text.lstrip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    texts = [word for word in tokens if word not in stopword_list]\n",
    "    texts = ' '.join(texts)\n",
    "    return texts\n",
    "\n",
    "def remove_punctuation_marks(text):\n",
    "    s = text\n",
    "    for c in string.punctuation:\n",
    "        s= s.replace(c,\"\")\n",
    "    return s\n",
    "    \n",
    "\n",
    "\n",
    "#process the data\n",
    "def pre_process_query(query1):\n",
    "        \n",
    "        #remove punctuation\n",
    "        query = remove_punctuation_marks(query1)\n",
    "        #remove stopwords\n",
    "        tweet = remove_stopwords(query)  \n",
    "        #Removes unicode strings like \"\\u002c\" and \"x96\"\n",
    "        try:\n",
    "           tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet)\n",
    "           tweet = re.sub(r'[^\\x00-\\x7f]',r'',tweet)\n",
    "        except:\n",
    "          tweet = tweet\n",
    "        #convert any url to URL\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "        #Convert any @Username to \"AT_USER\"\n",
    "        tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "        #Remove additional white spaces\n",
    "        tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "        tweet = re.sub('[\\n]+', ' ', tweet)\n",
    "        #Remove not alphanumeric symbols white spaces\n",
    "        tweet = re.sub(r'[^\\w]', ' ', tweet)\n",
    "        #Removes hastag in front of a word \"\"\"\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "        #Replace #word with word\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "        #lemma\n",
    "        from textblob import Word\n",
    "        tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n",
    "        #stemmer\n",
    "        st = PorterStemmer()\n",
    "        tweet=\" \".join([st.stem(word) for word in tweet.split()])\n",
    "        #Removes emoticons from text\n",
    "        tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', tweet)\n",
    "        #trim\n",
    "        tweet = tweet.strip('\\'\"')\n",
    "        row = tweet\n",
    "        return row\n",
    "\n",
    "#pre_process_query(query1='Dhoni has won the match against punjab team, and now he is leaving the stadium with his team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VlS57orrELq",
    "outputId": "9859ad02-8229-4527-d209-33cd632151fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-12 16:08:06--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.196.96\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.196.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  60.3MB/s    in 25s     \n",
      "\n",
      "2022-02-12 16:08:32 (61.8 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download pre-trained google word2vec model and save it into root path\n",
    "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4jBl23So8QI",
    "outputId": "8b6c12bb-ef23-4338-c529-d7a2580421ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True, limit=50000)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = text.replace('  ', ' ')\n",
    "    if text.startswith(\" \"):\n",
    "        text = text.lstrip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    texts = [word for word in tokens if word not in stopword_list]\n",
    "    return texts\n",
    "\n",
    "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
    "\n",
    "def get_embedding(word):\n",
    "    if word in model.vocab:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "    # Getting average vector for each document\n",
    "\n",
    "def get_vectors(fin):\n",
    "    out_dict = {}\n",
    "    for sen in fin:\n",
    "              average_vector = np.mean(np.array([get_embedding(x) for x in remove_stopwords(sen)]), axis=0)\n",
    "              dict = {sen: (average_vector)}\n",
    "              out_dict.update(dict)\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "# Function to calculate the similarity between the query vector and document vector\n",
    "def get_sim(query_embedding, average_vector_doc):\n",
    "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
    "    return sim\n",
    "\n",
    "\n",
    "# Rank all the documents based on the similarity to get relevant docs\n",
    "def Ranked_documents(query, corpse):\n",
    "    query_words = (\n",
    "        np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())], dtype=float), axis=0))\n",
    "    rank = []\n",
    "    fin = corpse\n",
    "    dict = get_vectors(fin)\n",
    "    for k, v in dict.items():\n",
    "      if not k=='':\n",
    "        rank.append((k, get_sim(query_words, v)))\n",
    "    rank = sorted(rank, key=lambda t: t[1], reverse=True)[:3]\n",
    "    return rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVRHg3CKtXkH",
    "outputId": "ecd2b47c-bd00-4845-a73f-25b67827daee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter your Query: Lahore won three matches straight in psl 2022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Lahore Qalandars have three big match-winners in the bowling department. The presence of Shaheen Afridi, Haris Rauf and Rashid Khan makes them a team to beat. The trio performed well last year and would look to dish out match-winning performances this year as well.',\n",
       "  [0.6878191772910041]),\n",
       " ('Fakhar Zaman is a Pakistani cricketer who plays for the Pakistan national cricket team and for Lahore Qalandars in the Pakistan Super League. On 20 July 2018, he became the first batsman for Pakistan to score a double century in a One Day International match.',\n",
       "  [0.5413623713820245]),\n",
       " ('Mohammad Rizwan is a Pakistani international cricketer who has represented Pakistan in international cricket since 2015. A right-handed batsman and wicket-keeper, he has scored centuries in all three international formats: Tests, One Day Internationals and Twenty20 Internationals.',\n",
       "  [0.48728765167582366])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "user_query = input (\"Please Enter your Query: \") \n",
    "Ranked_documents(user_query,[\"\",\n",
    "                             \"Shahid Afridi, is a Pakistani cricketer and the former captain of the Pakistan national cricket team. Afridi is widely considered as one of the most popular and destructive cricketers. An all-rounder, Afridi bowled leg spin and was recognized for his aggressive batting.\",\n",
    "                             \"Mohammad Rizwan is a Pakistani international cricketer who has represented Pakistan in international cricket since 2015. A right-handed batsman and wicket-keeper, he has scored centuries in all three international formats: Tests, One Day Internationals and Twenty20 Internationals.\",\n",
    "                             \"Fakhar Zaman is a Pakistani cricketer who plays for the Pakistan national cricket team and for Lahore Qalandars in the Pakistan Super League. On 20 July 2018, he became the first batsman for Pakistan to score a double century in a One Day International match.\",\n",
    "                             \"Lahore Qalandars have three big match-winners in the bowling department. The presence of Shaheen Afridi, Haris Rauf and Rashid Khan makes them a team to beat. The trio performed well last year and would look to dish out match-winning performances this year as well.\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Information_Retrival.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
